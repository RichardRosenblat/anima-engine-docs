# ADR-009: Configurable AI Model Topology with Mandatory Cortex and Optional Arcuate (NLP)

**Status:** Accepted
**Date:** 2026-01-05
**Deciders:** @RichardRosenblat
**Context:** Resource constraints, modular cognition, NLP delegation, hardware variability

---

## Context

ANIMA requires AI-driven reasoning to operate as a long-lived, task-oriented cognitive system.
At minimum, this includes:

* Cognitive planning and task expansion
* Contingency reasoning
* Capability-aware decision making
* Adherence to identity seeds and behavioral constraints

These responsibilities are **fundamental to ANIMA’s existence** and cannot be optional.

Separately, ANIMA also interacts with natural language:

* Parsing user input
* Producing conversational responses
* Generating semantic frames
* Supporting language-driven modules

Originally, Natural Language Processing (NLP) was conceived as a **module-level responsibility**, where individual modules could bring their own language models or parsers.

As the system evolved, several pressures emerged:

* Some deployments require a **centralized, consistent NLP layer**
* Other deployments must remain minimal due to hardware constraints
* NLP must not automatically imply access to memory or cognition
* Modules should not be forced to embed heavy AI dependencies

This created the need for a **configurable NLP responsibility**, without compromising the core cognitive engine.

---

## Decision

ANIMA adopts a **two-role AI model topology**:

1. **Cortex** — mandatory, responsible for cognition
2. **Arcuate** — optional, responsible for natural language processing

The Core MUST always provide a Cortex.
The Core MAY provide an Arcuate.

Natural Language Processing MAY be:

* Handled centrally by the Core via Arcuate, or
* Delegated entirely to NLP-capable modules

The Core exposes both responsibilities via stable interfaces, regardless of how they are implemented.

---

## Definition

### Cortex (Mandatory)

The **Cortex** is the cognitive model wrapper and is a required component of the ANIMA Core.

The Cortex is responsible for:

* Plan generation and refinement
* Task expansion and contingency modeling
* Semantic spine reasoning
* Capability-aware decision making
* Enforcing seed-defined behavioral constraints
* Reasoning over controlled memory slices

Without a Cortex, ANIMA cannot function as a cognitive system.

---

### Arcuate (Optional)

**Arcuate** is the natural language processing model wrapper.

Arcuate MAY be provided by the Core to handle:

* Natural language parsing
* Semantic frame extraction
* Natural language generation
* Linguistic normalization and realization

Arcuate MUST NOT:

* Perform autonomous planning
* Generate or mutate tasks
* Access episodic or narrative memory
* Override Cortex decisions

---

### NLP as a Configurable Responsibility

NLP processing in ANIMA can exist in two forms:

1. **Core-level Arcuate**

   * NLP is centralized
   * Ensures consistency in language handling
   * Reduces duplication across modules
   * Subject to strict memory and context isolation

2. **Module-level NLP**

   * Modules implement their own NLP capabilities
   * Core Arcuate slot is not used
   * Modules may use lightweight or specialized NLP tools
   * Core remains language-agnostic beyond cognition

This represents a deliberate shift from **NLP as a module-only concern** to **NLP as a configurable Core responsibility**.

---

## Interface-Based Architecture

The Core defines stable interfaces, such as:

* `ICognitive` → implemented by Cortex
* `INLP` → implemented by Arcuate or by modules

The Core and modules depend only on interfaces, never on concrete models.

This ensures:

* Model agnosticism
* Hot-swappable implementations
* Safe delegation of responsibilities

---

## Supported Configurations

### 1. Single Model, Dual Role (Cortex + Arcuate)

* One AI model implements both interfaces
* Operates in explicitly declared modes
* Cortex mode has access to controlled memory slices
* Arcuate mode operates with restricted or empty memory
* Ideal for low-resource environments

---

### 2. Dual Model Core (Dedicated Cortex + Dedicated Arcuate)

* Separate models for cognition and NLP
* Allows specialization and parallelism
* Suitable for high-resource deployments

---

### 3. Cortex-Only Core

* Cortex is present and mandatory
* Arcuate slot is empty
* NLP is handled entirely by modules
* Core provides no centralized language processing

---

## Constraints and Invariants

The following rules are architectural invariants:

* Cortex MUST always be present
* Arcuate is optional and replaceable
* Modules MUST NOT access Cortex directly
* All reasoning access is mediated by the Core
* NLP calls MUST NOT implicitly access memory
* Cognitive calls receive only validated memory slices
* Language understanding does not imply cognitive authority

These constraints preserve safety, clarity of responsibility, and identity integrity.

---

## Consequences

### Positive

* Guarantees a stable cognitive core in all deployments
* Allows NLP to scale independently of cognition
* Supports minimal, modular, and high-performance configurations
* Prevents memory leakage through language processing
* Keeps modules lightweight and optional
* Clarifies responsibility boundaries between reasoning and language

---

### Trade-offs

* Requires explicit routing of language responsibilities
* Adds configuration complexity at deployment time
* Increases importance of interface discipline
* Requires careful prompt and context construction per role

These trade-offs are intentional and aligned with ANIMA’s kernel-oriented design.

---

## Rationale

This decision formalizes a critical insight:

> **Cognition is essential. Language is a capability.**

By making Cortex mandatory and Arcuate optional:

* ANIMA remains fundamentally a thinking system
* Language becomes an attachable interface, not a defining trait
* Resource constraints no longer dictate architectural compromise
* Safety and memory boundaries remain enforceable by design

---

## Future Work

* Define strict memory visibility rules for Arcuate
* Formalize “Lite NLP” guarantees for module usage
* Add configuration profiles for Cortex-only vs Cortex+Arcuate deployments
* Support runtime switching between module-level and Core-level NLP
* Document best practices for NLP-capable modules
